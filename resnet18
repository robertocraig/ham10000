digraph {
	graph [size="59.55,59.55"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139637157530576 [label="
 (1, 2)" fillcolor=darkolivegreen1]
	139637158819344 [label=AddmmBackward0]
	139637158819008 -> 139637158819344
	139637164614624 [label="model.fc.bias
 (2)" fillcolor=lightblue]
	139637164614624 -> 139637158819008
	139637158819008 [label=AccumulateGrad]
	139637158819104 -> 139637158819344
	139637158819104 [label=ViewBackward0]
	139637158819152 -> 139637158819104
	139637158819152 [label=MeanBackward1]
	139637158818864 -> 139637158819152
	139637158818864 [label=ReluBackward0]
	139637158818768 -> 139637158818864
	139637158818768 [label=AddBackward0]
	139637158818672 -> 139637158818768
	139637158818672 [label=CudnnBatchNormBackward0]
	139637158818384 -> 139637158818672
	139637158818384 [label=ConvolutionBackward0]
	139637158818192 -> 139637158818384
	139637158818192 [label=ReluBackward0]
	139637158817952 -> 139637158818192
	139637158817952 [label=CudnnBatchNormBackward0]
	139637158817856 -> 139637158817952
	139637158817856 [label=ConvolutionBackward0]
	139637158818720 -> 139637158817856
	139637158818720 [label=ReluBackward0]
	139637158819584 -> 139637158818720
	139637158819584 [label=AddBackward0]
	139637158819680 -> 139637158819584
	139637158819680 [label=CudnnBatchNormBackward0]
	139637158819824 -> 139637158819680
	139637158819824 [label=ConvolutionBackward0]
	139637158820016 -> 139637158819824
	139637158820016 [label=ReluBackward0]
	139637158820160 -> 139637158820016
	139637158820160 [label=CudnnBatchNormBackward0]
	139637158820256 -> 139637158820160
	139637158820256 [label=ConvolutionBackward0]
	139637158820448 -> 139637158820256
	139637158820448 [label=ReluBackward0]
	139637158820592 -> 139637158820448
	139637158820592 [label=AddBackward0]
	139637158820688 -> 139637158820592
	139637158820688 [label=CudnnBatchNormBackward0]
	139637158820832 -> 139637158820688
	139637158820832 [label=ConvolutionBackward0]
	139637158821024 -> 139637158820832
	139637158821024 [label=ReluBackward0]
	139637158821168 -> 139637158821024
	139637158821168 [label=CudnnBatchNormBackward0]
	139637158821264 -> 139637158821168
	139637158821264 [label=ConvolutionBackward0]
	139637158820640 -> 139637158821264
	139637158820640 [label=ReluBackward0]
	139637158821552 -> 139637158820640
	139637158821552 [label=AddBackward0]
	139637158821648 -> 139637158821552
	139637158821648 [label=CudnnBatchNormBackward0]
	139637158821792 -> 139637158821648
	139637158821792 [label=ConvolutionBackward0]
	139632935895200 -> 139637158821792
	139632935895200 [label=ReluBackward0]
	139632935895344 -> 139632935895200
	139632935895344 [label=CudnnBatchNormBackward0]
	139632935895440 -> 139632935895344
	139632935895440 [label=ConvolutionBackward0]
	139632935895632 -> 139632935895440
	139632935895632 [label=ReluBackward0]
	139632935895776 -> 139632935895632
	139632935895776 [label=AddBackward0]
	139632935895872 -> 139632935895776
	139632935895872 [label=CudnnBatchNormBackward0]
	139632935896016 -> 139632935895872
	139632935896016 [label=ConvolutionBackward0]
	139632935896208 -> 139632935896016
	139632935896208 [label=ReluBackward0]
	139632935896352 -> 139632935896208
	139632935896352 [label=CudnnBatchNormBackward0]
	139632935896448 -> 139632935896352
	139632935896448 [label=ConvolutionBackward0]
	139632935895824 -> 139632935896448
	139632935895824 [label=ReluBackward0]
	139632935896736 -> 139632935895824
	139632935896736 [label=AddBackward0]
	139632935896832 -> 139632935896736
	139632935896832 [label=CudnnBatchNormBackward0]
	139632935896976 -> 139632935896832
	139632935896976 [label=ConvolutionBackward0]
	139632935897168 -> 139632935896976
	139632935897168 [label=ReluBackward0]
	139632935897312 -> 139632935897168
	139632935897312 [label=CudnnBatchNormBackward0]
	139632935897408 -> 139632935897312
	139632935897408 [label=ConvolutionBackward0]
	139632935897600 -> 139632935897408
	139632935897600 [label=ReluBackward0]
	139632935897744 -> 139632935897600
	139632935897744 [label=AddBackward0]
	139632935897840 -> 139632935897744
	139632935897840 [label=CudnnBatchNormBackward0]
	139632935897984 -> 139632935897840
	139632935897984 [label=ConvolutionBackward0]
	139632935898176 -> 139632935897984
	139632935898176 [label=ReluBackward0]
	139632935898320 -> 139632935898176
	139632935898320 [label=CudnnBatchNormBackward0]
	139632935898416 -> 139632935898320
	139632935898416 [label=ConvolutionBackward0]
	139632935897792 -> 139632935898416
	139632935897792 [label=ReluBackward0]
	139632935898704 -> 139632935897792
	139632935898704 [label=AddBackward0]
	139632935898800 -> 139632935898704
	139632935898800 [label=CudnnBatchNormBackward0]
	139632935898944 -> 139632935898800
	139632935898944 [label=ConvolutionBackward0]
	139632935899136 -> 139632935898944
	139632935899136 [label=ReluBackward0]
	139632935899280 -> 139632935899136
	139632935899280 [label=CudnnBatchNormBackward0]
	139632935899376 -> 139632935899280
	139632935899376 [label=ConvolutionBackward0]
	139632935898752 -> 139632935899376
	139632935898752 [label=MaxPool2DWithIndicesBackward0]
	139632935899664 -> 139632935898752
	139632935899664 [label=ReluBackward0]
	139632935899760 -> 139632935899664
	139632935899760 [label=CudnnBatchNormBackward0]
	139632935899856 -> 139632935899760
	139632935899856 [label=ConvolutionBackward0]
	139632935900048 -> 139632935899856
	139637157384640 [label="model.conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	139637157384640 -> 139632935900048
	139632935900048 [label=AccumulateGrad]
	139632935899808 -> 139632935899760
	139637157159584 [label="model.bn1.weight
 (64)" fillcolor=lightblue]
	139637157159584 -> 139632935899808
	139632935899808 [label=AccumulateGrad]
	139632935899472 -> 139632935899760
	139637157388400 [label="model.bn1.bias
 (64)" fillcolor=lightblue]
	139637157388400 -> 139632935899472
	139632935899472 [label=AccumulateGrad]
	139632935899568 -> 139632935899376
	139637157179008 [label="model.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139637157179008 -> 139632935899568
	139632935899568 [label=AccumulateGrad]
	139632935899328 -> 139632935899280
	139637157179408 [label="model.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139637157179408 -> 139632935899328
	139632935899328 [label=AccumulateGrad]
	139632935899184 -> 139632935899280
	139637157178928 [label="model.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139637157178928 -> 139632935899184
	139632935899184 [label=AccumulateGrad]
	139632935899088 -> 139632935898944
	139637157173568 [label="model.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139637157173568 -> 139632935899088
	139632935899088 [label=AccumulateGrad]
	139632935898896 -> 139632935898800
	139637157173408 [label="model.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139637157173408 -> 139632935898896
	139632935898896 [label=AccumulateGrad]
	139632935898848 -> 139632935898800
	139637157173648 [label="model.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139637157173648 -> 139632935898848
	139632935898848 [label=AccumulateGrad]
	139632935898752 -> 139632935898704
	139632935898608 -> 139632935898416
	139637157179488 [label="model.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139637157179488 -> 139632935898608
	139632935898608 [label=AccumulateGrad]
	139632935898368 -> 139632935898320
	139637157173968 [label="model.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139637157173968 -> 139632935898368
	139632935898368 [label=AccumulateGrad]
	139632935898224 -> 139632935898320
	139637157179728 [label="model.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139637157179728 -> 139632935898224
	139632935898224 [label=AccumulateGrad]
	139632935898128 -> 139632935897984
	139637157176688 [label="model.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139637157176688 -> 139632935898128
	139632935898128 [label=AccumulateGrad]
	139632935897936 -> 139632935897840
	139637157177248 [label="model.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139637157177248 -> 139632935897936
	139632935897936 [label=AccumulateGrad]
	139632935897888 -> 139632935897840
	139637157176608 [label="model.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139637157176608 -> 139632935897888
	139632935897888 [label=AccumulateGrad]
	139632935897792 -> 139632935897744
	139632935897552 -> 139632935897408
	139637157180208 [label="model.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	139637157180208 -> 139632935897552
	139632935897552 [label=AccumulateGrad]
	139632935897360 -> 139632935897312
	139637157180048 [label="model.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139637157180048 -> 139632935897360
	139632935897360 [label=AccumulateGrad]
	139632935897216 -> 139632935897312
	139637157179888 [label="model.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139637157179888 -> 139632935897216
	139632935897216 [label=AccumulateGrad]
	139632935897120 -> 139632935896976
	139637157180928 [label="model.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139637157180928 -> 139632935897120
	139632935897120 [label=AccumulateGrad]
	139632935896928 -> 139632935896832
	139637157180528 [label="model.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139637157180528 -> 139632935896928
	139632935896928 [label=AccumulateGrad]
	139632935896880 -> 139632935896832
	139637157182368 [label="model.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139637157182368 -> 139632935896880
	139632935896880 [label=AccumulateGrad]
	139632935896784 -> 139632935896736
	139632935896784 [label=CudnnBatchNormBackward0]
	139632935897504 -> 139632935896784
	139632935897504 [label=ConvolutionBackward0]
	139632935897600 -> 139632935897504
	139632935897648 -> 139632935897504
	139637157175408 [label="model.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	139637157175408 -> 139632935897648
	139632935897648 [label=AccumulateGrad]
	139632935897072 -> 139632935896784
	139637157174528 [label="model.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	139637157174528 -> 139632935897072
	139632935897072 [label=AccumulateGrad]
	139632935897024 -> 139632935896784
	139637157179808 [label="model.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	139637157179808 -> 139632935897024
	139632935897024 [label=AccumulateGrad]
	139632935896640 -> 139632935896448
	139637157181648 [label="model.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139637157181648 -> 139632935896640
	139632935896640 [label=AccumulateGrad]
	139632935896400 -> 139632935896352
	139637157181408 [label="model.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139637157181408 -> 139632935896400
	139632935896400 [label=AccumulateGrad]
	139632935896256 -> 139632935896352
	139637157181888 [label="model.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139637157181888 -> 139632935896256
	139632935896256 [label=AccumulateGrad]
	139632935896160 -> 139632935896016
	139637157170208 [label="model.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139637157170208 -> 139632935896160
	139632935896160 [label=AccumulateGrad]
	139632935895968 -> 139632935895872
	139637157169808 [label="model.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139637157169808 -> 139632935895968
	139632935895968 [label=AccumulateGrad]
	139632935895920 -> 139632935895872
	139637157170688 [label="model.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139637157170688 -> 139632935895920
	139632935895920 [label=AccumulateGrad]
	139632935895824 -> 139632935895776
	139632935895584 -> 139632935895440
	139637158412048 [label="model.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	139637158412048 -> 139632935895584
	139632935895584 [label=AccumulateGrad]
	139632935895392 -> 139632935895344
	139637158407648 [label="model.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139637158407648 -> 139632935895392
	139632935895392 [label=AccumulateGrad]
	139632935895248 -> 139632935895344
	139637158405168 [label="model.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139637158405168 -> 139632935895248
	139632935895248 [label=AccumulateGrad]
	139632935895152 -> 139637158821792
	139637164445984 [label="model.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139637164445984 -> 139632935895152
	139632935895152 [label=AccumulateGrad]
	139637158821744 -> 139637158821648
	139637164443984 [label="model.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139637164443984 -> 139637158821744
	139637158821744 [label=AccumulateGrad]
	139637158821696 -> 139637158821648
	139637164446784 [label="model.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139637164446784 -> 139637158821696
	139637158821696 [label=AccumulateGrad]
	139637158821600 -> 139637158821552
	139637158821600 [label=CudnnBatchNormBackward0]
	139637158821840 -> 139637158821600
	139637158821840 [label=ConvolutionBackward0]
	139632935895632 -> 139637158821840
	139632935895680 -> 139637158821840
	139637157170528 [label="model.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	139637157170528 -> 139632935895680
	139632935895680 [label=AccumulateGrad]
	139632935895536 -> 139637158821600
	139637157182448 [label="model.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	139637157182448 -> 139632935895536
	139632935895536 [label=AccumulateGrad]
	139632935895104 -> 139637158821600
	139637157172688 [label="model.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	139637157172688 -> 139632935895104
	139632935895104 [label=AccumulateGrad]
	139637158821456 -> 139637158821264
	139637157313760 [label="model.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139637157313760 -> 139637158821456
	139637158821456 [label=AccumulateGrad]
	139637158821216 -> 139637158821168
	139637157313840 [label="model.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139637157313840 -> 139637158821216
	139637158821216 [label=AccumulateGrad]
	139637158821072 -> 139637158821168
	139637157314000 [label="model.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139637157314000 -> 139637158821072
	139637158821072 [label=AccumulateGrad]
	139637158820976 -> 139637158820832
	139637157312560 [label="model.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139637157312560 -> 139637158820976
	139637158820976 [label=AccumulateGrad]
	139637158820784 -> 139637158820688
	139637157313280 [label="model.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139637157313280 -> 139637158820784
	139637158820784 [label=AccumulateGrad]
	139637158820736 -> 139637158820688
	139637157313200 [label="model.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139637157313200 -> 139637158820736
	139637158820736 [label=AccumulateGrad]
	139637158820640 -> 139637158820592
	139637158820400 -> 139637158820256
	139637157310960 [label="model.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	139637157310960 -> 139637158820400
	139637158820400 [label=AccumulateGrad]
	139637158820208 -> 139637158820160
	139637157311200 [label="model.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139637157311200 -> 139637158820208
	139637158820208 [label=AccumulateGrad]
	139637158820064 -> 139637158820160
	139637157311280 [label="model.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139637157311280 -> 139637158820064
	139637158820064 [label=AccumulateGrad]
	139637158819968 -> 139637158819824
	139637157310320 [label="model.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139637157310320 -> 139637158819968
	139637158819968 [label=AccumulateGrad]
	139637158819776 -> 139637158819680
	139637157310480 [label="model.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139637157310480 -> 139637158819776
	139637158819776 [label=AccumulateGrad]
	139637158819728 -> 139637158819680
	139637157310000 [label="model.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139637157310000 -> 139637158819728
	139637158819728 [label=AccumulateGrad]
	139637158819632 -> 139637158819584
	139637158819632 [label=CudnnBatchNormBackward0]
	139637158820352 -> 139637158819632
	139637158820352 [label=ConvolutionBackward0]
	139637158820448 -> 139637158820352
	139637158820496 -> 139637158820352
	139637157312240 [label="model.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139637157312240 -> 139637158820496
	139637158820496 [label=AccumulateGrad]
	139637158819920 -> 139637158819632
	139637157311760 [label="model.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	139637157311760 -> 139637158819920
	139637158819920 [label=AccumulateGrad]
	139637158819872 -> 139637158819632
	139637157312000 [label="model.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	139637157312000 -> 139637158819872
	139637158819872 [label=AccumulateGrad]
	139637158817568 -> 139637158817856
	139637157309760 [label="model.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139637157309760 -> 139637158817568
	139637158817568 [label=AccumulateGrad]
	139637158817904 -> 139637158817952
	139637157309840 [label="model.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139637157309840 -> 139637158817904
	139637158817904 [label=AccumulateGrad]
	139637158818144 -> 139637158817952
	139637157309680 [label="model.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139637157309680 -> 139637158818144
	139637158818144 [label=AccumulateGrad]
	139637158818240 -> 139637158818384
	139637157308800 [label="model.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139637157308800 -> 139637158818240
	139637158818240 [label=AccumulateGrad]
	139637158818432 -> 139637158818672
	139637157308320 [label="model.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139637157308320 -> 139637158818432
	139637158818432 [label=AccumulateGrad]
	139637158818624 -> 139637158818672
	139637157308880 [label="model.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139637157308880 -> 139637158818624
	139637158818624 [label=AccumulateGrad]
	139637158818720 -> 139637158818768
	139637158819056 -> 139637158819344
	139637158819056 [label=TBackward0]
	139637158818816 -> 139637158819056
	139637164621664 [label="model.fc.weight
 (2, 512)" fillcolor=lightblue]
	139637164621664 -> 139637158818816
	139637158818816 [label=AccumulateGrad]
	139637158819344 -> 139637157530576
}
